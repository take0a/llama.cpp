# llama.cpp

![llama](https://user-images.githubusercontent.com/1991296/230134379-7181e485-c521-4d23-a0d6-f7b3b61ba524.png)

[![License: MIT](https://img.shields.io/badge/license-MIT-blue.svg)](https://opensource.org/licenses/MIT)
[![Release](https://img.shields.io/github/v/release/ggml-org/llama.cpp)](https://github.com/ggml-org/llama.cpp/releases)
[![Server](https://github.com/ggml-org/llama.cpp/actions/workflows/server.yml/badge.svg)](https://github.com/ggml-org/llama.cpp/actions/workflows/server.yml)

[Roadmap](https://github.com/users/ggerganov/projects/7) / [Manifesto](https://github.com/ggml-org/llama.cpp/discussions/205) / [ggml](https://github.com/ggml-org/ggml)

Metaã®[LLaMA](https://arxiv.org/abs/2302.13971)ãƒ¢ãƒ‡ãƒ«ï¼ˆãŠã‚ˆã³ãã®ä»–ï¼‰ã®ç´”ç²‹ãªC/C++ã§ã®æ¨è«–

## æœ€è¿‘ã®APIã®å¤‰æ›´

- [Changelog for `libllama` API](https://github.com/ggml-org/llama.cpp/issues/9289)
- [Changelog for `llama-server` REST API](https://github.com/ggml-org/llama.cpp/issues/9291)

## æ³¨ç›®ãƒˆãƒ”ãƒƒã‚¯

- ğŸ”¥ `llama-server` ã«ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã‚µãƒãƒ¼ãƒˆãŒè¿½åŠ ã•ã‚Œã¾ã—ãŸ: [#12898](https://github.com/ggml-org/llama.cpp/pull/12898) | [ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](./docs/multimodal.md)
- æ–°ã—ã„ãƒã‚¤ãƒŠãƒª `llama-mtmd-cli` ãŒå°å…¥ã•ã‚Œã¾ã—ãŸã€‚ã“ã‚Œã¯ã€`llava-cli`ã€`minicpmv-cli`ã€`gemma3-cli` ([#13012](https://github.com/ggml-org/llama.cpp/pull/13012))ã€`qwen2vl-cli` ([#13141](https://github.com/ggml-org/llama.cpp/pull/13141)) ã®ä»£æ›¿ã¨ãªã‚Šã¾ã™ã€‚`libllava` ã¯éæ¨å¥¨ã¨ãªã‚Šã¾ã™ã€‚
- FIM è£œå®Œç”¨ã® VS Code æ‹¡å¼µæ©Ÿèƒ½: https://github.com/ggml-org/llama.vscode
- `llama-server` ã«ãŠã‘ã‚‹ãƒ¦ãƒ‹ãƒãƒ¼ã‚µãƒ« [ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã‚µãƒãƒ¼ãƒˆ](./docs/function-calling.md) https://github.com/ggml-org/llama.cpp/pull/9639
- FIMè£œå®Œç”¨Vim/Neovimãƒ—ãƒ©ã‚°ã‚¤ãƒ³: https://github.com/ggml-org/llama.vim
- GGUF-my-LoRAã®ã”ç´¹ä»‹ https://github.com/ggml-org/llama.cpp/discussions/10123
- Hugging Faceæ¨è«–ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆãŒGGUFã‚’æ¨™æº–ã‚µãƒãƒ¼ãƒˆã—ã¾ã—ãŸï¼ https://github.com/ggml-org/llama.cpp/discussions/9669
- Hugging Face GGUFã‚¨ãƒ‡ã‚£ã‚¿: [ãƒ‡ã‚£ã‚¹ã‚«ãƒƒã‚·ãƒ§ãƒ³](https://github.com/ggml-org/llama.cpp/discussions/9268) | [ãƒ„ãƒ¼ãƒ«](https://huggingface.co/spaces/CISCai/gguf-editor)

----

## ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ

llama.cpp ã®ä½¿ã„æ–¹ã¯ç°¡å˜ã§ã™ã€‚ãŠä½¿ã„ã®ãƒã‚·ãƒ³ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹æ–¹æ³•ã¯ã„ãã¤ã‹ã‚ã‚Šã¾ã™ã€‚

- [brewã€nixã€ã¾ãŸã¯ winget](docs/install.md) ã‚’ä½¿ç”¨ã—ã¦ `llama.cpp` ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹
- Docker ã§å®Ÿè¡Œã™ã‚‹ - [Docker ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](docs/docker.md) ã‚’ã”è¦§ãã ã•ã„ã€‚
- [ãƒªãƒªãƒ¼ã‚¹ãƒšãƒ¼ã‚¸](https://github.com/ggml-org/llama.cpp/releases) ã‹ã‚‰ãƒ“ãƒ«ãƒ‰æ¸ˆã¿ã®ãƒã‚¤ãƒŠãƒªã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹
- ã“ã®ãƒªãƒã‚¸ãƒˆãƒªã‚’ã‚¯ãƒ­ãƒ¼ãƒ³ã—ã¦ã‚½ãƒ¼ã‚¹ã‹ã‚‰ãƒ“ãƒ«ãƒ‰ã™ã‚‹ - [ãƒ“ãƒ«ãƒ‰ã‚¬ã‚¤ãƒ‰](docs/build.md) ã‚’ã”è¦§ãã ã•ã„ã€‚

ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãŒå®Œäº†ã—ãŸã‚‰ã€ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ãŒå¿…è¦ã«ãªã‚Šã¾ã™ã€‚è©³ç´°ã«ã¤ã„ã¦ã¯ã€[ãƒ¢ãƒ‡ãƒ«ã®å–å¾—ã¨é‡å­åŒ–](#obtaining-and-quantizing-models) ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ã”è¦§ãã ã•ã„ã€‚

ã‚³ãƒãƒ³ãƒ‰ä¾‹:

```sh
# Use a local model file
llama-cli -m my_model.gguf

# Or download and run a model directly from Hugging Face
llama-cli -hf ggml-org/gemma-3-1b-it-GGUF

# Launch OpenAI-compatible API server
llama-server -hf ggml-org/gemma-3-1b-it-GGUF
```

## èª¬æ˜

`llama.cpp` ã®ä¸»ãªç›®çš„ã¯ã€æœ€å°é™ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã§ã€ãƒ­ãƒ¼ã‚«ãƒ«ãŠã‚ˆã³ã‚¯ãƒ©ã‚¦ãƒ‰ä¸Šã®å¹…åºƒã„ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ä¸Šã§æœ€å…ˆç«¯ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å®Ÿç¾ã—ã€LLM æ¨è«–ã‚’å®Ÿç¾ã™ã‚‹ã“ã¨ã§ã™ã€‚

- ä¾å­˜é–¢ä¿‚ã®ãªã„ã‚·ãƒ³ãƒ—ãƒ«ãªC/C++å®Ÿè£…
- Apple Siliconã‚’ç¬¬ä¸€ç´šå¸‚æ°‘ã¨ã—ã¦æ¡ç”¨ - ARM NEONã€Accelerateã€Metalãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§æœ€é©åŒ–
- x86ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å‘ã‘AVXã€AVX2ã€AVX512ã€AMXã‚µãƒãƒ¼ãƒˆ
- 1.5ãƒ“ãƒƒãƒˆã€2ãƒ“ãƒƒãƒˆã€3ãƒ“ãƒƒãƒˆã€4ãƒ“ãƒƒãƒˆã€5ãƒ“ãƒƒãƒˆã€6ãƒ“ãƒƒãƒˆã€8ãƒ“ãƒƒãƒˆã®æ•´æ•°é‡å­åŒ–ã«ã‚ˆã‚Šã€æ¨è«–é€Ÿåº¦ã®å‘ä¸Šã¨ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®å‰Šæ¸›ã‚’å®Ÿç¾
- NVIDIA GPUã§LLMã‚’å®Ÿè¡Œã™ã‚‹ãŸã‚ã®ã‚«ã‚¹ã‚¿ãƒ CUDAã‚«ãƒ¼ãƒãƒ«ï¼ˆHIPçµŒç”±ã®AMD GPUã¨MUSAçµŒç”±ã®ãƒ ãƒ¼ã‚¢ã‚¹ãƒ¬ãƒƒãƒ‰GPUã‚’ã‚µãƒãƒ¼ãƒˆï¼‰
- VulkanãŠã‚ˆã³SYCLãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚’ã‚µãƒãƒ¼ãƒˆ
- VRAMå®¹é‡å…¨ä½“ã‚’è¶…ãˆã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’éƒ¨åˆ†çš„ã«é«˜é€ŸåŒ–ã™ã‚‹CPU+GPUãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¨è«–

`llama.cpp`ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯ã€[ggml](https://github.com/ggml-org/ggml)ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®æ–°æ©Ÿèƒ½é–‹ç™ºã®ãŸã‚ã®ä¸»è¦ãªãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã§ã™ã€‚

<details>
<summary>Models</summary>

é€šå¸¸ã€ä»¥ä¸‹ã®åŸºæœ¬ãƒ¢ãƒ‡ãƒ«ã®å¾®èª¿æ•´ã‚‚ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã™ã€‚

æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ã®ã‚µãƒãƒ¼ãƒˆã‚’è¿½åŠ ã™ã‚‹æ‰‹é †: [HOWTO-add-model.md](docs/development/HOWTO-add-model.md)

#### Text-only

- [X] LLaMA ğŸ¦™
- [x] LLaMA 2 ğŸ¦™ğŸ¦™
- [x] LLaMA 3 ğŸ¦™ğŸ¦™ğŸ¦™
- [X] [Mistral 7B](https://huggingface.co/mistralai/Mistral-7B-v0.1)
- [x] [Mixtral MoE](https://huggingface.co/models?search=mistral-ai/Mixtral)
- [x] [DBRX](https://huggingface.co/databricks/dbrx-instruct)
- [X] [Falcon](https://huggingface.co/models?search=tiiuae/falcon)
- [X] [Chinese LLaMA / Alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca) and [Chinese LLaMA-2 / Alpaca-2](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2)
- [X] [Vigogne (French)](https://github.com/bofenghuang/vigogne)
- [X] [BERT](https://github.com/ggml-org/llama.cpp/pull/5423)
- [X] [Koala](https://bair.berkeley.edu/blog/2023/04/03/koala/)
- [X] [Baichuan 1 & 2](https://huggingface.co/models?search=baichuan-inc/Baichuan) + [derivations](https://huggingface.co/hiyouga/baichuan-7b-sft)
- [X] [Aquila 1 & 2](https://huggingface.co/models?search=BAAI/Aquila)
- [X] [Starcoder models](https://github.com/ggml-org/llama.cpp/pull/3187)
- [X] [Refact](https://huggingface.co/smallcloudai/Refact-1_6B-fim)
- [X] [MPT](https://github.com/ggml-org/llama.cpp/pull/3417)
- [X] [Bloom](https://github.com/ggml-org/llama.cpp/pull/3553)
- [x] [Yi models](https://huggingface.co/models?search=01-ai/Yi)
- [X] [StableLM models](https://huggingface.co/stabilityai)
- [x] [Deepseek models](https://huggingface.co/models?search=deepseek-ai/deepseek)
- [x] [Qwen models](https://huggingface.co/models?search=Qwen/Qwen)
- [x] [PLaMo-13B](https://github.com/ggml-org/llama.cpp/pull/3557)
- [x] [Phi models](https://huggingface.co/models?search=microsoft/phi)
- [x] [PhiMoE](https://github.com/ggml-org/llama.cpp/pull/11003)
- [x] [GPT-2](https://huggingface.co/gpt2)
- [x] [Orion 14B](https://github.com/ggml-org/llama.cpp/pull/5118)
- [x] [InternLM2](https://huggingface.co/models?search=internlm2)
- [x] [CodeShell](https://github.com/WisdomShell/codeshell)
- [x] [Gemma](https://ai.google.dev/gemma)
- [x] [Mamba](https://github.com/state-spaces/mamba)
- [x] [Grok-1](https://huggingface.co/keyfan/grok-1-hf)
- [x] [Xverse](https://huggingface.co/models?search=xverse)
- [x] [Command-R models](https://huggingface.co/models?search=CohereForAI/c4ai-command-r)
- [x] [SEA-LION](https://huggingface.co/models?search=sea-lion)
- [x] [GritLM-7B](https://huggingface.co/GritLM/GritLM-7B) + [GritLM-8x7B](https://huggingface.co/GritLM/GritLM-8x7B)
- [x] [OLMo](https://allenai.org/olmo)
- [x] [OLMo 2](https://allenai.org/olmo)
- [x] [OLMoE](https://huggingface.co/allenai/OLMoE-1B-7B-0924)
- [x] [Granite models](https://huggingface.co/collections/ibm-granite/granite-code-models-6624c5cec322e4c148c8b330)
- [x] [GPT-NeoX](https://github.com/EleutherAI/gpt-neox) + [Pythia](https://github.com/EleutherAI/pythia)
- [x] [Snowflake-Arctic MoE](https://huggingface.co/collections/Snowflake/arctic-66290090abe542894a5ac520)
- [x] [Smaug](https://huggingface.co/models?search=Smaug)
- [x] [Poro 34B](https://huggingface.co/LumiOpen/Poro-34B)
- [x] [Bitnet b1.58 models](https://huggingface.co/1bitLLM)
- [x] [Flan T5](https://huggingface.co/models?search=flan-t5)
- [x] [Open Elm models](https://huggingface.co/collections/apple/openelm-instruct-models-6619ad295d7ae9f868b759ca)
- [x] [ChatGLM3-6b](https://huggingface.co/THUDM/chatglm3-6b) + [ChatGLM4-9b](https://huggingface.co/THUDM/glm-4-9b) + [GLMEdge-1.5b](https://huggingface.co/THUDM/glm-edge-1.5b-chat) + [GLMEdge-4b](https://huggingface.co/THUDM/glm-edge-4b-chat)
- [x] [GLM-4-0414](https://huggingface.co/collections/THUDM/glm-4-0414-67f3cbcb34dd9d252707cb2e)
- [x] [SmolLM](https://huggingface.co/collections/HuggingFaceTB/smollm-6695016cad7167254ce15966)
- [x] [EXAONE-3.0-7.8B-Instruct](https://huggingface.co/LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct)
- [x] [FalconMamba Models](https://huggingface.co/collections/tiiuae/falconmamba-7b-66b9a580324dd1598b0f6d4a)
- [x] [Jais](https://huggingface.co/inceptionai/jais-13b-chat)
- [x] [Bielik-11B-v2.3](https://huggingface.co/collections/speakleash/bielik-11b-v23-66ee813238d9b526a072408a)
- [x] [RWKV-6](https://github.com/BlinkDL/RWKV-LM)
- [x] [QRWKV-6](https://huggingface.co/recursal/QRWKV6-32B-Instruct-Preview-v0.1)
- [x] [GigaChat-20B-A3B](https://huggingface.co/ai-sage/GigaChat-20B-A3B-instruct)
- [X] [Trillion-7B-preview](https://huggingface.co/trillionlabs/Trillion-7B-preview)
- [x] [Ling models](https://huggingface.co/collections/inclusionAI/ling-67c51c85b34a7ea0aba94c32)

#### Multimodal

- [x] [LLaVA 1.5 models](https://huggingface.co/collections/liuhaotian/llava-15-653aac15d994e992e2677a7e), [LLaVA 1.6 models](https://huggingface.co/collections/liuhaotian/llava-16-65b9e40155f60fd046a5ccf2)
- [x] [BakLLaVA](https://huggingface.co/models?search=SkunkworksAI/Bakllava)
- [x] [Obsidian](https://huggingface.co/NousResearch/Obsidian-3B-V0.5)
- [x] [ShareGPT4V](https://huggingface.co/models?search=Lin-Chen/ShareGPT4V)
- [x] [MobileVLM 1.7B/3B models](https://huggingface.co/models?search=mobileVLM)
- [x] [Yi-VL](https://huggingface.co/models?search=Yi-VL)
- [x] [Mini CPM](https://huggingface.co/models?search=MiniCPM)
- [x] [Moondream](https://huggingface.co/vikhyatk/moondream2)
- [x] [Bunny](https://github.com/BAAI-DCAI/Bunny)
- [x] [GLM-EDGE](https://huggingface.co/models?search=glm-edge)
- [x] [Qwen2-VL](https://huggingface.co/collections/Qwen/qwen2-vl-66cee7455501d7126940800d)

</details>

<details>
<summary>Bindings</summary>

- Python: [ddh0/easy-llama](https://github.com/ddh0/easy-llama)
- Python: [abetlen/llama-cpp-python](https://github.com/abetlen/llama-cpp-python)
- Go: [go-skynet/go-llama.cpp](https://github.com/go-skynet/go-llama.cpp)
- Node.js: [withcatai/node-llama-cpp](https://github.com/withcatai/node-llama-cpp)
- JS/TS (llama.cpp server client): [lgrammel/modelfusion](https://modelfusion.dev/integration/model-provider/llamacpp)
- JS/TS (Programmable Prompt Engine CLI): [offline-ai/cli](https://github.com/offline-ai/cli)
- JavaScript/Wasm (works in browser): [tangledgroup/llama-cpp-wasm](https://github.com/tangledgroup/llama-cpp-wasm)
- Typescript/Wasm (nicer API, available on npm): [ngxson/wllama](https://github.com/ngxson/wllama)
- Ruby: [yoshoku/llama_cpp.rb](https://github.com/yoshoku/llama_cpp.rb)
- Rust (more features): [edgenai/llama_cpp-rs](https://github.com/edgenai/llama_cpp-rs)
- Rust (nicer API): [mdrokz/rust-llama.cpp](https://github.com/mdrokz/rust-llama.cpp)
- Rust (more direct bindings): [utilityai/llama-cpp-rs](https://github.com/utilityai/llama-cpp-rs)
- Rust (automated build from crates.io): [ShelbyJenkins/llm_client](https://github.com/ShelbyJenkins/llm_client)
- C#/.NET: [SciSharp/LLamaSharp](https://github.com/SciSharp/LLamaSharp)
- C#/VB.NET (more features - community license): [LM-Kit.NET](https://docs.lm-kit.com/lm-kit-net/index.html)
- Scala 3: [donderom/llm4s](https://github.com/donderom/llm4s)
- Clojure: [phronmophobic/llama.clj](https://github.com/phronmophobic/llama.clj)
- React Native: [mybigday/llama.rn](https://github.com/mybigday/llama.rn)
- Java: [kherud/java-llama.cpp](https://github.com/kherud/java-llama.cpp)
- Zig: [deins/llama.cpp.zig](https://github.com/Deins/llama.cpp.zig)
- Flutter/Dart: [netdur/llama_cpp_dart](https://github.com/netdur/llama_cpp_dart)
- Flutter: [xuegao-tzx/Fllama](https://github.com/xuegao-tzx/Fllama)
- PHP (API bindings and features built on top of llama.cpp): [distantmagic/resonance](https://github.com/distantmagic/resonance) [(more info)](https://github.com/ggml-org/llama.cpp/pull/6326)
- Guile Scheme: [guile_llama_cpp](https://savannah.nongnu.org/projects/guile-llama-cpp)
- Swift [srgtuszy/llama-cpp-swift](https://github.com/srgtuszy/llama-cpp-swift)
- Swift [ShenghaiWang/SwiftLlama](https://github.com/ShenghaiWang/SwiftLlama)
- Delphi [Embarcadero/llama-cpp-delphi](https://github.com/Embarcadero/llama-cpp-delphi)

</details>

<details>
<summary>UIs</summary>

*(to have a project listed here, it should clearly state that it depends on `llama.cpp`)*

- [AI Sublime Text plugin](https://github.com/yaroslavyaroslav/OpenAI-sublime-text) (MIT)
- [cztomsik/ava](https://github.com/cztomsik/ava) (MIT)
- [Dot](https://github.com/alexpinel/Dot) (GPL)
- [eva](https://github.com/ylsdamxssjxxdd/eva) (MIT)
- [iohub/collama](https://github.com/iohub/coLLaMA) (Apache-2.0)
- [janhq/jan](https://github.com/janhq/jan) (AGPL)
- [johnbean393/Sidekick](https://github.com/johnbean393/Sidekick) (MIT)
- [KanTV](https://github.com/zhouwg/kantv?tab=readme-ov-file) (Apache-2.0)
- [KodiBot](https://github.com/firatkiral/kodibot) (GPL)
- [llama.vim](https://github.com/ggml-org/llama.vim) (MIT)
- [LARS](https://github.com/abgulati/LARS) (AGPL)
- [Llama Assistant](https://github.com/vietanhdev/llama-assistant) (GPL)
- [LLMFarm](https://github.com/guinmoon/LLMFarm?tab=readme-ov-file) (MIT)
- [LLMUnity](https://github.com/undreamai/LLMUnity) (MIT)
- [LMStudio](https://lmstudio.ai/) (proprietary)
- [LocalAI](https://github.com/mudler/LocalAI) (MIT)
- [LostRuins/koboldcpp](https://github.com/LostRuins/koboldcpp) (AGPL)
- [MindMac](https://mindmac.app) (proprietary)
- [MindWorkAI/AI-Studio](https://github.com/MindWorkAI/AI-Studio) (FSL-1.1-MIT)
- [Mobile-Artificial-Intelligence/maid](https://github.com/Mobile-Artificial-Intelligence/maid) (MIT)
- [Mozilla-Ocho/llamafile](https://github.com/Mozilla-Ocho/llamafile) (Apache-2.0)
- [nat/openplayground](https://github.com/nat/openplayground) (MIT)
- [nomic-ai/gpt4all](https://github.com/nomic-ai/gpt4all) (MIT)
- [ollama/ollama](https://github.com/ollama/ollama) (MIT)
- [oobabooga/text-generation-webui](https://github.com/oobabooga/text-generation-webui) (AGPL)
- [PocketPal AI](https://github.com/a-ghorbani/pocketpal-ai) (MIT)
- [psugihara/FreeChat](https://github.com/psugihara/FreeChat) (MIT)
- [ptsochantaris/emeltal](https://github.com/ptsochantaris/emeltal) (MIT)
- [pythops/tenere](https://github.com/pythops/tenere) (AGPL)
- [ramalama](https://github.com/containers/ramalama) (MIT)
- [semperai/amica](https://github.com/semperai/amica) (MIT)
- [withcatai/catai](https://github.com/withcatai/catai) (MIT)
- [Autopen](https://github.com/blackhole89/autopen) (GPL)

</details>

<details>
<summary>Tools</summary>

- [akx/ggify](https://github.com/akx/ggify) â€“ download PyTorch models from HuggingFace Hub and convert them to GGML
- [akx/ollama-dl](https://github.com/akx/ollama-dl) â€“ download models from the Ollama library to be used directly with llama.cpp
- [crashr/gppm](https://github.com/crashr/gppm) â€“ launch llama.cpp instances utilizing NVIDIA Tesla P40 or P100 GPUs with reduced idle power consumption
- [gpustack/gguf-parser](https://github.com/gpustack/gguf-parser-go/tree/main/cmd/gguf-parser) - review/check the GGUF file and estimate the memory usage
- [Styled Lines](https://marketplace.unity.com/packages/tools/generative-ai/styled-lines-llama-cpp-model-292902) (proprietary licensed, async wrapper of inference part for game development in Unity3d with pre-built Mobile and Web platform wrappers and a model example)

</details>

<details>
<summary>Infrastructure</summary>

- [Paddler](https://github.com/distantmagic/paddler) - Stateful load balancer custom-tailored for llama.cpp
- [GPUStack](https://github.com/gpustack/gpustack) - Manage GPU clusters for running LLMs
- [llama_cpp_canister](https://github.com/onicai/llama_cpp_canister) - llama.cpp as a smart contract on the Internet Computer, using WebAssembly
- [llama-swap](https://github.com/mostlygeek/llama-swap) - transparent proxy that adds automatic model switching with llama-server
- [Kalavai](https://github.com/kalavai-net/kalavai-client) - Crowdsource end to end LLM deployment at any scale
- [llmaz](https://github.com/InftyAI/llmaz) - â˜¸ï¸ Easy, advanced inference platform for large language models on Kubernetes.
</details>

<details>
<summary>Games</summary>

- [Lucy's Labyrinth](https://github.com/MorganRO8/Lucys_Labyrinth) - A simple maze game where agents controlled by an AI model will try to trick you.

</details>


## Supported backends

| Backend | Target devices |
| --- | --- |
| [Metal](docs/build.md#metal-build) | Apple Silicon |
| [BLAS](docs/build.md#blas-build) | All |
| [BLIS](docs/backend/BLIS.md) | All |
| [SYCL](docs/backend/SYCL.md) | Intel and Nvidia GPU |
| [MUSA](docs/build.md#musa) | Moore Threads GPU |
| [CUDA](docs/build.md#cuda) | Nvidia GPU |
| [HIP](docs/build.md#hip) | AMD GPU |
| [Vulkan](docs/build.md#vulkan) | GPU |
| [CANN](docs/build.md#cann) | Ascend NPU |
| [OpenCL](docs/backend/OPENCL.md) | Adreno GPU |
| [RPC](https://github.com/ggml-org/llama.cpp/tree/master/tools/rpc) | All |

## ãƒ¢ãƒ‡ãƒ«ã®å–å¾—ã¨é‡å­åŒ–

[Hugging Face](https://huggingface.co) ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã¯ã€`llama.cpp` ã¨äº’æ›æ€§ã®ã‚ã‚‹ [å¤šæ•°ã® LLM](https://huggingface.co/models?library=gguf&sort=trending) ã‚’ãƒ›ã‚¹ãƒˆã—ã¦ã„ã¾ã™ã€‚

- [Trending](https://huggingface.co/models?library=gguf&sort=trending)
- [LLaMA](https://huggingface.co/models?sort=trending&search=llama+gguf)

GGUF ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ‰‹å‹•ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã‹ã€[Hugging Face](https://huggingface.co/) ã¾ãŸã¯ [ModelScope](https://modelscope.cn/) ãªã©ã®ãƒ¢ãƒ‡ãƒ«ãƒ›ã‚¹ãƒ†ã‚£ãƒ³ã‚°ã‚µã‚¤ãƒˆã‹ã‚‰ `llama.cpp` äº’æ›ãƒ¢ãƒ‡ãƒ«ã‚’ç›´æ¥ä½¿ç”¨ã§ãã¾ã™ã€‚ãã®å ´åˆã¯ã€CLI å¼•æ•° `-hf <user>/<model>[:quant]` ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚ä¾‹:

```sh
llama-cli -hf ggml-org/gemma-3-1b-it-GGUF
```

ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯ã€CLIã¯Hugging Faceã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ãŒã€ç’°å¢ƒå¤‰æ•°`MODEL_ENDPOINT`ã‚’ä½¿ç”¨ã—ã¦ä»–ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã«åˆ‡ã‚Šæ›¿ãˆã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ä¾‹ãˆã°ã€ç’°å¢ƒå¤‰æ•°`MODEL_ENDPOINT=https://www.modelscope.cn/`ã‚’è¨­å®šã™ã‚‹ã“ã¨ã§ã€ModelScopeã‚„ãã®ä»–ã®ãƒ¢ãƒ‡ãƒ«å…±æœ‰ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã‚ˆã†ã«é¸æŠã§ãã¾ã™ã€‚

ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸã‚‰ã€CLIãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¦ãƒ­ãƒ¼ã‚«ãƒ«ã§å®Ÿè¡Œã—ã¾ã™ã€‚è©³ç´°ã¯ä»¥ä¸‹ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

`llama.cpp`ã‚’ä½¿ç”¨ã™ã‚‹ã«ã¯ã€ãƒ¢ãƒ‡ãƒ«ã‚’[GGUF](https://github.com/ggml-org/ggml/blob/master/docs/gguf.md)ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã§ä¿å­˜ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ä»–ã®ãƒ‡ãƒ¼ã‚¿å½¢å¼ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€ã“ã®ãƒªãƒã‚¸ãƒˆãƒªã«ã‚ã‚‹`convert_*.py` Pythonã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½¿ç”¨ã—ã¦GGUFã«å¤‰æ›ã§ãã¾ã™ã€‚

Hugging Face ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã¯ã€`llama.cpp` ã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’å¤‰æ›ã€é‡å­åŒ–ã€ãƒ›ã‚¹ãƒ†ã‚£ãƒ³ã‚°ã™ã‚‹ãŸã‚ã®ã•ã¾ã–ã¾ãªã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒ„ãƒ¼ãƒ«ã‚’æä¾›ã—ã¦ã„ã¾ã™ã€‚

- [GGUF-my-repo ã‚¹ãƒšãƒ¼ã‚¹](https://huggingface.co/spaces/ggml-org/gguf-my-repo) ã‚’ä½¿ç”¨ã—ã¦ã€GGUF å½¢å¼ã«å¤‰æ›ã—ã€ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’ã‚ˆã‚Šå°ã•ãªã‚µã‚¤ã‚ºã«é‡å­åŒ–ã—ã¾ã™ã€‚
- [GGUF-my-LoRA ã‚¹ãƒšãƒ¼ã‚¹](https://huggingface.co/spaces/ggml-org/gguf-my-lora) ã‚’ä½¿ç”¨ã—ã¦ã€LoRA ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’ GGUF å½¢å¼ã«å¤‰æ›ã—ã¾ã™ (è©³ç´°ã¯ã“ã¡ã‚‰: https://github.com/ggml-org/llama.cpp/discussions/10123)ã€‚
- [GGUF-editor ã‚¹ãƒšãƒ¼ã‚¹](https://huggingface.co/spaces/CISCai/gguf-editor) ã‚’ä½¿ç”¨ã—ã¦ã€ãƒ–ãƒ©ã‚¦ã‚¶ã§ GGUF ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ç·¨é›†ã—ã¾ã™ (è©³ç´°ã¯ã“ã¡ã‚‰: https://github.com/ggml-org/llama.cpp/discussions/9268)
- [æ¨è«–ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ](https://ui.endpoints.huggingface.co/) ã‚’ä½¿ç”¨ã—ã¦ã€ã‚¯ãƒ©ã‚¦ãƒ‰ã§ `llama.cpp` ã‚’ç›´æ¥ãƒ›ã‚¹ãƒˆã—ã¾ã™ (è©³ç´°: https://github.com/ggml-org/llama.cpp/discussions/9669)

ãƒ¢ãƒ‡ãƒ«ã®é‡å­åŒ–ã®è©³ç´°ã«ã¤ã„ã¦ã¯ã€[ã“ã¡ã‚‰ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](tools/quantize/README.md) ã‚’ã”è¦§ãã ã•ã„

## [`llama-cli`](tools/main)

#### `llama.cpp` ã®ã»ã¨ã‚“ã©ã®æ©Ÿèƒ½ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦å®Ÿé¨“ã™ã‚‹ãŸã‚ã® CLI ãƒ„ãƒ¼ãƒ«ã€‚

- <details open>
    <summary>ä¼šè©±ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œ</summary>

    ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãŒçµ„ã¿è¾¼ã¾ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã¯ã€è‡ªå‹•çš„ã«ä¼šè©±ãƒ¢ãƒ¼ãƒ‰ã‚’æœ‰åŠ¹ã«ã—ã¾ã™ã€‚æœ‰åŠ¹ã«ãªã‚‰ãªã„å ´åˆã¯ã€`-cnv` ã‚’è¿½åŠ ã—ã€`--chat-template NAME` ã§é©åˆ‡ãªãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’æŒ‡å®šã™ã‚‹ã“ã¨ã§ã€æ‰‹å‹•ã§æœ‰åŠ¹ã«ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

    ```bash
    llama-cli -m model.gguf

    # > hi, who are you?
    # Hi there! I'm your helpful assistant! I'm an AI-powered chatbot designed to assist and provide information to users like you. I'm here to help answer your questions, provide guidance, and offer support on a wide range of topics. I'm a friendly and knowledgeable AI, and I'm always happy to help with anything you need. What's on your mind, and how can I assist you today?
    #
    # > what is 1+1?
    # Easy peasy! The answer to 1+1 is... 2!
    ```

    </details>

- <details>
    <summary>ã‚«ã‚¹ã‚¿ãƒ ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½¿ç”¨ã—ã¦ä¼šè©±ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã™ã‚‹</summary>

    ```bash
    # use the "chatml" template (use -h to see the list of supported templates)
    llama-cli -m model.gguf -cnv --chat-template chatml

    # use a custom template
    llama-cli -m model.gguf -cnv --in-prefix 'User: ' --reverse-prompt 'User:'
    ```

    </details>

- <details>
    <summary>ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ†ã‚­ã‚¹ãƒˆè£œå®Œã‚’å®Ÿè¡Œã™ã‚‹</summary>

    ä¼šè©±ãƒ¢ãƒ¼ãƒ‰ã‚’æ˜ç¤ºçš„ã«ç„¡åŠ¹ã«ã™ã‚‹ã«ã¯ã€`-no-cnv` ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚

    ```bash
    llama-cli -m model.gguf -p "I believe the meaning of life is" -n 128 -no-cnv

    # I believe the meaning of life is to find your own truth and to live in accordance with it. For me, this means being true to myself and following my passions, even if they don't align with societal expectations. I think that's what I love about yoga â€“ it's not just a physical practice, but a spiritual one too. It's about connecting with yourself, listening to your inner voice, and honoring your own unique journey.
    ```

    </details>

- <details>
    <summary>ã‚«ã‚¹ã‚¿ãƒ æ–‡æ³•ã§å‡ºåŠ›ã‚’åˆ¶é™ã™ã‚‹</summary>

    ```bash
    llama-cli -m model.gguf -n 256 --grammar-file grammars/json.gbnf -p 'Request: schedule a call at 8pm; Command:'

    # {"appointmentTime": "8pm", "appointmentDetails": "schedule a a call"}
    ```

    [grammars/](grammars/) ãƒ•ã‚©ãƒ«ãƒ€ã«ã¯ã€ã„ãã¤ã‹ã®ã‚µãƒ³ãƒ—ãƒ«æ–‡æ³•ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚ç‹¬è‡ªã®æ–‡æ³•ã‚’ä½œæˆã™ã‚‹ã«ã¯ã€[GBNF ã‚¬ã‚¤ãƒ‰](grammars/README.md) ã‚’ã”è¦§ãã ã•ã„ã€‚

    ã‚ˆã‚Šè¤‡é›‘ãª JSON æ–‡æ³•ã‚’ä½œæˆã™ã‚‹ã«ã¯ã€https://grammar.intrinsiclabs.ai/ ã‚’ã”è¦§ãã ã•ã„ã€‚

    </details>


## [`llama-server`](tools/server)

#### LLM ã‚’æä¾›ã™ã‚‹ãŸã‚ã®è»½é‡ã® [OpenAI API](https://github.com/openai/openai-openapi) äº’æ› HTTP ã‚µãƒ¼ãƒãƒ¼ã€‚

- <details open>
    <summary>ãƒãƒ¼ãƒˆ8080ã§ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã®ãƒ­ãƒ¼ã‚«ãƒ«HTTPã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹•ã—ã¾ã™ã€‚</summary>

    ```bash
    llama-server -m model.gguf --port 8080

    # Basic web UI can be accessed via browser: http://localhost:8080
    # Chat completion endpoint: http://localhost:8080/v1/chat/completions
    ```

    </details>

- <details>
    <summary>è¤‡æ•°ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨ä¸¦åˆ—ãƒ‡ã‚³ãƒ¼ãƒ‰ã‚’ã‚µãƒãƒ¼ãƒˆ</summary>

    ```bash
    # up to 4 concurrent requests, each with 4096 max context
    llama-server -m model.gguf -c 16384 -np 4
    ```

    </details>

- <details>
    <summary>æŠ•æ©Ÿçš„ãƒ‡ã‚³ãƒ¼ãƒ‰ã‚’æœ‰åŠ¹ã«ã™ã‚‹</summary>

    ```bash
    # the draft.gguf model should be a small variant of the target model.gguf
    llama-server -m model.gguf -md draft.gguf
    ```

    </details>

- <details>
    <summary>åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚’æä¾›ã™ã‚‹</summary>

    ```bash
    # use the /embedding endpoint
    llama-server -m model.gguf --embedding --pooling cls -ub 8192
    ```

    </details>

- <details>
    <summary>å†ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã‚’æä¾›ã™ã‚‹</summary>

    ```bash
    # use the /reranking endpoint
    llama-server -m model.gguf --reranking
    ```

    </details>

- <details>
    <summary>ã™ã¹ã¦ã®å‡ºåŠ›ã‚’æ–‡æ³•ã§åˆ¶ç´„ã™ã‚‹</summary>

    ```bash
    # custom grammar
    llama-server -m model.gguf --grammar-file grammar.gbnf

    # JSON
    llama-server -m model.gguf --grammar-file grammars/json.gbnf
    ```

    </details>


## [`llama-perplexity`](tools/perplexity)

#### ä¸ãˆã‚‰ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã«å¯¾ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã®å›°æƒ‘åº¦[^1][^2]ï¼ˆãŠã‚ˆã³ãã®ä»–ã®å“è³ªæŒ‡æ¨™ï¼‰ã‚’æ¸¬å®šã™ã‚‹ãŸã‚ã®ãƒ„ãƒ¼ãƒ«ã€‚

- <details open>
    <summary>ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®å›°æƒ‘åº¦ã‚’æ¸¬å®šã™ã‚‹</summary>

    ```bash
    llama-perplexity -m model.gguf -f file.txt

    # [1]15.2701,[2]5.4007,[3]5.3073,[4]6.2965,[5]5.8940,[6]5.6096,[7]5.7942,[8]4.9297, ...
    # Final estimate: PPL = 5.4007 +/- 0.67339
    ```

    </details>

- <details>
    <summary>KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã‚’æ¸¬å®šã™ã‚‹</summary>

    ```bash
    # TODO
    ```

    </details>

[^1]: [tools/perplexity/README.md](./tools/perplexity/README.md)
[^2]: [https://huggingface.co/docs/transformers/perplexity](https://huggingface.co/docs/transformers/perplexity)

## [`llama-bench`](tools/llama-bench)

#### ã•ã¾ã–ã¾ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹æ¨è«–ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã—ã¾ã™ã€‚

- <details open>
    <summary>ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å®Ÿè¡Œã™ã‚‹</summary>

    ```bash
    llama-bench -m model.gguf

    # Output:
    # | model               |       size |     params | backend    | threads |          test |                  t/s |
    # | ------------------- | ---------: | ---------: | ---------- | ------: | ------------: | -------------------: |
    # | qwen2 1.5B Q4_0     | 885.97 MiB |     1.54 B | Metal,BLAS |      16 |         pp512 |      5765.41 Â± 20.55 |
    # | qwen2 1.5B Q4_0     | 885.97 MiB |     1.54 B | Metal,BLAS |      16 |         tg128 |        197.71 Â± 0.81 |
    #
    # build: 3e0ba0e60 (4229)
    ```

    </details>

## [`llama-run`](tools/run)

#### `llama.cpp` ãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè¡Œã™ã‚‹ãŸã‚ã®åŒ…æ‹¬çš„ãªä¾‹ã€‚æ¨è«–ã«ä¾¿åˆ©ã§ã™ã€‚RamaLama [^3] ã§ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚

- <details>
    <summary>ç‰¹å®šã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè¡Œã—ã¾ã™ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯Ollamaãƒ¬ã‚¸ã‚¹ãƒˆãƒªã‹ã‚‰å–å¾—ã•ã‚Œã¾ã™ï¼‰</summary>

    ```bash
    llama-run granite-code
    ```

    </details>

[^3]: [RamaLama](https://github.com/containers/ramalama)

## [`llama-simple`](examples/simple)

#### `llama.cpp` ã‚’ä½¿ã£ã¦ã‚¢ãƒ—ãƒªã‚’å®Ÿè£…ã™ã‚‹ãŸã‚ã®æœ€å°é™ã®ä¾‹ã€‚é–‹ç™ºè€…ã«ã¨ã£ã¦ä¾¿åˆ©ã§ã™ã€‚

- <details>
    <summary>åŸºæœ¬çš„ãªãƒ†ã‚­ã‚¹ãƒˆè£œå®Œ</summary>

    ```bash
    llama-simple -m model.gguf

    # Hello my name is Kaitlyn and I am a 16 year old girl. I am a junior in high school and I am currently taking a class called "The Art of
    ```

    </details>


## è²¢çŒ®

- è²¢çŒ®è€…ã¯ãƒ—ãƒ«ãƒªã‚¯ã‚¨ã‚¹ãƒˆï¼ˆPRï¼‰ã‚’é–‹ãã“ã¨ãŒã§ãã¾ã™ã€‚
- ã‚³ãƒ©ãƒœãƒ¬ãƒ¼ã‚¿ãƒ¼ã¯ `llama.cpp` ãƒªãƒã‚¸ãƒˆãƒªã®ãƒ–ãƒ©ãƒ³ãƒã«ãƒ—ãƒƒã‚·ãƒ¥ã—ã€ãƒ—ãƒ«ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ `master` ãƒ–ãƒ©ãƒ³ãƒã«ãƒãƒ¼ã‚¸ã§ãã¾ã™ã€‚
- ã‚³ãƒ©ãƒœãƒ¬ãƒ¼ã‚¿ãƒ¼ã¯è²¢çŒ®åº¦ã«åŸºã¥ã„ã¦æ‹›å¾…ã•ã‚Œã¾ã™ã€‚
- å•é¡Œã€ãƒ—ãƒ«ãƒªã‚¯ã‚¨ã‚¹ãƒˆã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ç®¡ç†ã«é–¢ã™ã‚‹ã”å”åŠ›ã‚’ã„ãŸã ã‘ã‚Œã°å¹¸ã„ã§ã™ã€‚
- æœ€åˆã®ã‚³ãƒ³ãƒˆãƒªãƒ“ãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ã«é©ã—ãŸã‚¿ã‚¹ã‚¯ã«ã¤ã„ã¦ã¯ã€[good first issues](https://github.com/ggml-org/llama.cpp/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22) ã‚’ã”è¦§ãã ã•ã„ã€‚
- è©³ç´°ã«ã¤ã„ã¦ã¯ã€[CONTRIBUTING.md](CONTRIBUTING.md) ã‚’ã”è¦§ãã ã•ã„ã€‚
- ã“ã¡ã‚‰ã‚‚å¿…ãšãŠèª­ã¿ãã ã•ã„: [Inference at the edge](https://github.com/ggml-org/llama.cpp/discussions/205)
- ã”èˆˆå‘³ã®ã‚ã‚‹æ–¹ã®ãŸã‚ã«ã€å°‘ã—èƒŒæ™¯ã‚’ãŠä¼ãˆã—ã¾ã™: [Changelog podcast](https://changelog.com/podcast/532)

## ãã®ä»–ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

- [ãƒ¡ã‚¤ãƒ³ (cli)](tools/main/README.md)
- [ã‚µãƒ¼ãƒãƒ¼](tools/server/README.md)
- [GBNF æ–‡æ³•](grammars/README.md)

#### é–‹ç™ºãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

- [ãƒ“ãƒ«ãƒ‰æ–¹æ³•](docs/build.md)
- [Docker ã§ã®å®Ÿè¡Œ](docs/docker.md)
- [Android ã§ã®ãƒ“ãƒ«ãƒ‰](docs/android.md)
- [ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°](docs/development/token_generation_performance_tips.md)
- [GGML ã®ãƒ’ãƒ³ãƒˆã¨ã‚³ãƒ„](https://github.com/ggml-org/llama.cpp/wiki/GGML-Tips-&-Tricks)

#### é‡è¦ãªè«–æ–‡ã¨ãƒ¢ãƒ‡ãƒ«ã®èƒŒæ™¯

ãƒ¢ãƒ‡ãƒ«ç”Ÿæˆã®å“è³ªã«é–¢ã™ã‚‹å•é¡ŒãŒã‚ã‚‹å ´åˆã¯ã€å°‘ãªãã¨ã‚‚ä»¥ä¸‹ã®ãƒªãƒ³ã‚¯ã¨è«–æ–‡ã‚’å‚ç…§ã—ã¦ã€LLaMAãƒ¢ãƒ‡ãƒ«ã®é™ç•Œã‚’ç†è§£ã—ã¦ãã ã•ã„ã€‚ã“ã‚Œã¯ã€é©åˆ‡ãªãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã‚’é¸æŠã—ã€LLaMAãƒ¢ãƒ‡ãƒ«ã¨ChatGPTãƒ¢ãƒ‡ãƒ«é–“ã®é‡è¦ãªé•ã„ã¨å¾®å¦™ãªé•ã„ã®ä¸¡æ–¹ã‚’ç†è§£ã™ã‚‹éš›ã«ç‰¹ã«é‡è¦ã§ã™ã€‚
- LLaMA:
- [LLaMAã®ç´¹ä»‹ï¼šåŸºç¤çš„ãª650å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/)
- [LLaMAï¼šã‚ªãƒ¼ãƒ—ãƒ³ã§åŠ¹ç‡çš„ãªåŸºç¤è¨€èªãƒ¢ãƒ‡ãƒ«](https://arxiv.org/abs/2302.13971)
- GPT-3
- [è¨€èªãƒ¢ãƒ‡ãƒ«ã¯å°‘æ•°ã‚·ãƒ§ãƒƒãƒˆå­¦ç¿’å™¨ã§ã™](https://arxiv.org/abs/2005.14165)
- GPT-3.5 / InstructGPT / ChatGPT:
- [æŒ‡ç¤ºã«å¾“ã†è¨€èªãƒ¢ãƒ‡ãƒ«ã®ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆ](https://openai.com/research/instruction-following)
- [äººé–“ã«ã‚ˆã‚‹æŒ‡ç¤ºã«å¾“ã†è¨€èªãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯](https://arxiv.org/abs/2203.02155)

## XCFramework
XCFramework ã¯ã€iOSã€visionOSã€tvOSã€macOS å‘ã‘ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ãƒ—ãƒªã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ç‰ˆã§ã™ã€‚Swift ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã¯ã€ã‚½ãƒ¼ã‚¹ã‹ã‚‰ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã™ã‚‹ã“ã¨ãªãä½¿ç”¨ã§ãã¾ã™ã€‚ä¾‹:
```swift
// swift-tools-version: 5.10
// The swift-tools-version declares the minimum version of Swift required to build this package.

import PackageDescription

let package = Package(
    name: "MyLlamaPackage",
    targets: [
        .executableTarget(
            name: "MyLlamaPackage",
            dependencies: [
                "LlamaFramework"
            ]),
        .binaryTarget(
            name: "LlamaFramework",
            url: "https://github.com/ggml-org/llama.cpp/releases/download/b5046/llama-b5046-xcframework.zip",
            checksum: "c19be78b5f00d8d29a25da41042cb7afa094cbf6280a225abe614b03b20029ab"
        )
    ]
)
```
ä¸Šè¨˜ã®ä¾‹ã§ã¯ã€ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ä¸­é–“ãƒ“ãƒ«ãƒ‰ã€Œb5046ã€ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚URLã¨ãƒã‚§ãƒƒã‚¯ã‚µãƒ ã‚’å¤‰æ›´ã™ã‚‹ã“ã¨ã§ã€åˆ¥ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ä½¿ç”¨ã™ã‚‹ã‚ˆã†ã«å¤‰æ›´ã§ãã¾ã™ã€‚

## è£œå®Œ
ä¸€éƒ¨ã®ç’°å¢ƒã§ã¯ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³è£œå®ŒãŒåˆ©ç”¨ã§ãã¾ã™ã€‚

#### Bash Completion
```bash
$ build/bin/llama-cli --completion-bash > ~/.llama-completion.bash
$ source ~/.llama-completion.bash
```
ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¨ã—ã¦ã€`.bashrc` ã¾ãŸã¯ `.bash_profile` ã«ã“ã‚Œã‚’è¿½åŠ ã—ã¦è‡ªå‹•çš„ã«èª­ã¿è¾¼ã‚€ã“ã¨ã‚‚ã§ãã¾ã™ã€‚ä¾‹:
```console
$ echo "source ~/.llama-completion.bash" >> ~/.bashrc
```

## ä¾å­˜é–¢ä¿‚

- [yhirose/cpp-httplib](https://github.com/yhirose/cpp-httplib) - ã‚·ãƒ³ã‚°ãƒ«ãƒ˜ãƒƒãƒ€ãƒ¼HTTPã‚µãƒ¼ãƒãƒ¼ã€‚`llama-server`ã§ä½¿ç”¨ã€‚- MITãƒ©ã‚¤ã‚»ãƒ³ã‚¹
- [stb-image](https://github.com/nothings/stb) - ã‚·ãƒ³ã‚°ãƒ«ãƒ˜ãƒƒãƒ€ãƒ¼ç”»åƒãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ã€‚ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã‚µãƒ–ã‚·ã‚¹ãƒ†ãƒ ã§ä½¿ç”¨ã€‚- ãƒ‘ãƒ–ãƒªãƒƒã‚¯ãƒ‰ãƒ¡ã‚¤ãƒ³
- [nlohmann/json](https://github.com/nlohmann/json) - ã‚·ãƒ³ã‚°ãƒ«ãƒ˜ãƒƒãƒ€ãƒ¼JSONãƒ©ã‚¤ãƒ–ãƒ©ãƒªã€‚æ§˜ã€…ãªãƒ„ãƒ¼ãƒ«ã‚„ã‚µãƒ³ãƒ—ãƒ«ã§ä½¿ç”¨ã€‚- MITãƒ©ã‚¤ã‚»ãƒ³ã‚¹
- [minja](https://github.com/google/minja) - C++ã§æ›¸ã‹ã‚ŒãŸæœ€å°é™ã®Jinjaãƒ‘ãƒ¼ã‚µãƒ¼ã€‚æ§˜ã€…ãªãƒ„ãƒ¼ãƒ«ã‚„ã‚µãƒ³ãƒ—ãƒ«ã§ä½¿ç”¨ã€‚- MITãƒ©ã‚¤ã‚»ãƒ³ã‚¹
- [linenoise.cpp](./tools/run/linenoise.cpp/linenoise.cpp) - readlineã®ã‚ˆã†ãªè¡Œç·¨é›†æ©Ÿèƒ½ã‚’æä¾›ã™ã‚‹C++ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã€‚`llama-run`ã§ä½¿ç”¨ã€‚- BSDãƒ©ã‚¤ã‚»ãƒ³ã‚¹2æ¡é …ãƒ©ã‚¤ã‚»ãƒ³ã‚¹
- [curl](https://curl.se/) - æ§˜ã€…ãªãƒ„ãƒ¼ãƒ«ã‚„ã‚µãƒ³ãƒ—ãƒ«ã§ä½¿ç”¨ã•ã‚Œã‚‹ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚µã‚¤ãƒ‰URLè»¢é€ãƒ©ã‚¤ãƒ–ãƒ©ãƒª - [CURLãƒ©ã‚¤ã‚»ãƒ³ã‚¹](https://curl.se/docs/copyright.html)
- [miniaudio.h](https://github.com/mackron/miniaudio) - ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã‚µãƒ–ã‚·ã‚¹ãƒ†ãƒ ã§ä½¿ç”¨ã•ã‚Œã‚‹ã‚·ãƒ³ã‚°ãƒ«ãƒ˜ãƒƒãƒ€ãƒ¼ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ - ãƒ‘ãƒ–ãƒªãƒƒã‚¯ãƒ‰ãƒ¡ã‚¤ãƒ³
